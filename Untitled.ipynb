{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb761df9-1481-4b5e-92a7-43bb448d74c0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bba39af-339c-4b1f-b360-c0a7ba50b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86adf46a-0c39-4d16-bee9-dc749f66b44a",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5989b479-d041-48a0-8add-ccb826e5e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for reading one file\n",
    "def read_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        blocks = file.read().split(\"\\n@highlight\\n\")\n",
    "\n",
    "    story = blocks[0].replace(\"\\n\", \" \").strip()  # Remove newlines from the story\n",
    "\n",
    "    # Concatenate highlights into one string, separated by dots\n",
    "    highlights = '. '.join([blocks[i].replace(\"\\n\", \" \").strip() for i in range(1, len(blocks))])\n",
    "\n",
    "\n",
    "    return story, highlights\n",
    "\n",
    "# function for reading all the stories\n",
    "def read_all_stories(directories):\n",
    "    # Gather all .STORY file paths from the specified directories\n",
    "    file_paths = []\n",
    "    for directory in directories:\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".story\"):\n",
    "                file_paths.append(os.path.join(directory, filename))\n",
    "\n",
    "    data = []  # List to hold data before converting to DataFrame\n",
    "\n",
    "    # Read and process each .STORY file\n",
    "    for path in tqdm(file_paths, desc=\"Processing files\"):\n",
    "        try:\n",
    "            story, highlights = read_dataset(path)\n",
    "            data.append({'story': story, 'highlights': highlights})\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {path}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "734559bb-17ec-4eb6-92d8-d4f25c4af866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|████████████████████████████████████████████████████████| 21288/21288 [00:02<00:00, 7910.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Data for testing the model and evaluation\n",
    "# file_paths = ['Data/sample']\n",
    "# raw_data = read_all_stories(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "430762c7-24fc-46b3-a569-6fafe3cb6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_paths = ['Data/testread1', 'Data/testread2']\n",
    "# raw_data = read_all_stories(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8358a73-b106-4e73-90be-835b59e54b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['Data/cnn', 'Data/dailymail']\n",
    "raw_data = read_all_stories(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a911f7fd-17dc-48bc-8395-1cee208a785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8578760b-3203-4f0f-960d-e2eb62be1dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data.highlights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d65ba1b4-8bda-4a35-8b2f-b3b6f8bcdfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_rows, num_columns = raw_data.shape\n",
    "# print(f\"Number of stories: {num_rows}\")\n",
    "# print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8d000c-895e-4786-a550-c066bd4f71ae",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f053b7-e121-4f18-91fc-b27ab2340e8e",
   "metadata": {},
   "source": [
    "#### Pre-processing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c3acc0e-50ed-4227-8d2a-617ba5fbbcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DataPreprocessor:\n",
    "#     def __init__(self):\n",
    "#         self.tokenizer = Tokenizer(oov_token=\"<OOV>\", num_words=15500) # determined by word count analysis\n",
    "\n",
    "\n",
    "#     def clean_df(self, df):\n",
    "#         for i in tqdm(range(len(df)), desc=\"Cleaning Dataframe\"):\n",
    "#             df.at[i, 'story'] = self.clean_text(df.at[i, 'story'])\n",
    "#             df.at[i, 'highlights'] = [self.clean_text(h) for h in df.at[i, 'highlights']]\n",
    "#         return df\n",
    "\n",
    "#     def clean_text(self, text):\n",
    "#         lowercase_text = text.lower()\n",
    "#         cleaned_text = self.remove_special(lowercase_text)\n",
    "#         return cleaned_text\n",
    "    \n",
    "#     def remove_special(self, text):\n",
    "#         # Replace special characters with spaces but keep .,!,?\n",
    "#         cleaned_text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', ' ', text)\n",
    "#         # Replace multiple spaces with a single space\n",
    "#         cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "#         return cleaned_text\n",
    "\n",
    "#     def get_texts_list(self, df):\n",
    "#         # Convert the story column to a list\n",
    "#         story_texts = df['story'].tolist()\n",
    "#         # Flatten the list of lists in the highlights column\n",
    "#         story_texts = [text for text in tqdm(df['story'], desc=\"Text Concatenation\")]\n",
    "#         highlight_texts = [highlight for sublist in df['highlights'] for highlight in sublist]\n",
    "#         # Combine the two lists\n",
    "#         all_texts = story_texts + highlight_texts\n",
    "#         return all_texts\n",
    "\n",
    "#     def tokenize(self, df):\n",
    "#         # Will need tokenization parameters\n",
    "#         all_texts_list = self.get_texts_list(df)\n",
    "#         self.tokenizer.fit_on_texts(all_texts_list)\n",
    "        \n",
    "\n",
    "#     def df_to_seq(self, df):\n",
    "#         for i in tqdm(range(len(df)), desc=\"Tokenizing Dataframe\"):\n",
    "#             df.at[i, 'story'] = self.tokenizer.texts_to_sequences([df.at[i, 'story']])[0]\n",
    "#             highlights = df.at[i, 'highlights']\n",
    "             \n",
    "#             # Add start and end tokens to each highlight\n",
    "#             highlights = ['<start> ' + highlight + ' <end>' for highlight in highlights]\n",
    "        \n",
    "#             tokenized_highlights = [self.tokenizer.texts_to_sequences([highlight])[0] for highlight in highlights]\n",
    "#             df.at[i, 'highlights'] = tokenized_highlights\n",
    "#         return df\n",
    "\n",
    "\n",
    "#     def data_padding(self, df):\n",
    "#         max_story_len = 1250\n",
    "#         max_highlight_len = 50\n",
    "        \n",
    "#         for i in tqdm(range(len(df)), desc=\"Padding sequences\"):\n",
    "#             # Padding for story\n",
    "#             df.at[i, 'story'] = pad_sequences([df.at[i, 'story']], maxlen=max_story_len, padding='post', truncating='post')[0]\n",
    "            \n",
    "#             # Padding for highlights\n",
    "#             df.at[i, 'highlights'] = [pad_sequences([highlight], maxlen=max_highlight_len, padding='post', truncating='post')[0] for highlight in df.at[i, 'highlights']]\n",
    "            \n",
    "#         return df\n",
    "\n",
    "#     def data_splitter(self, df):\n",
    "#         X = df['story']\n",
    "#         y = df['highlights']\n",
    "\n",
    "#         # Splitting data into train, test and validation subsets\n",
    "#         X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.20, random_state=42, shuffle=True)\n",
    "#         X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "#         return X_train, y_train, X_val, X_test, y_val, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65e75c09-f093-43f0-ac1b-dec21b3581b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer(oov_token=\"<OOV>\", num_words=15500) # determined by word count analysis\n",
    "\n",
    "    def clean_df(self, df):\n",
    "        for i in tqdm(range(len(df)), desc=\"Cleaning Dataframe\"):\n",
    "            df.at[i, 'story'] = self.clean_text(df.at[i, 'story'])\n",
    "            df.at[i, 'highlights'] = self.clean_text(df.at[i, 'highlights'])\n",
    "        return df\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        lowercase_text = text.lower()\n",
    "        cleaned_text = self.remove_special(lowercase_text)\n",
    "        return cleaned_text\n",
    "    \n",
    "    def remove_special(self, text):\n",
    "        # Replace special characters with spaces but keep .,!,?\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', ' ', text)\n",
    "        # Replace multiple spaces with a single space\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "        return cleaned_text\n",
    "\n",
    "    def get_texts_list(self, df):\n",
    "        # Convert both columns to lists\n",
    "        story_texts = df['story'].tolist()\n",
    "        highlight_texts = df['highlights'].tolist()\n",
    "        # Combine the two lists\n",
    "        all_texts = story_texts + highlight_texts\n",
    "        return all_texts\n",
    "\n",
    "\n",
    "    def tokenize(self, df):\n",
    "        # Will need tokenization parameters\n",
    "        all_texts_list = self.get_texts_list(df)\n",
    "        self.tokenizer.fit_on_texts(all_texts_list)\n",
    "        \n",
    "\n",
    "    def df_to_seq(self, df):\n",
    "        for i in tqdm(range(len(df)), desc=\"Tokenizing Dataframe\"):\n",
    "            df.at[i, 'story'] = self.tokenizer.texts_to_sequences([df.at[i, 'story']])[0]\n",
    "            \n",
    "            # Add start and end tokens to the highlight\n",
    "            highlight = '<start> ' + df.at[i, 'highlights'] + ' <end>'\n",
    "            df.at[i, 'highlights'] = self.tokenizer.texts_to_sequences([highlight])[0]\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def data_padding(self, df):\n",
    "        max_story_len = 1800\n",
    "        max_highlight_len = 70  # Adjust this if needed since it's a single string now\n",
    "            \n",
    "        for i in tqdm(range(len(df)), desc=\"Padding sequences\"):\n",
    "            df.at[i, 'story'] = pad_sequences([df.at[i, 'story']], maxlen=max_story_len, padding='post', truncating='post')[0]\n",
    "            df.at[i, 'highlights'] = pad_sequences([df.at[i, 'highlights']], maxlen=max_highlight_len, padding='post', truncating='post')[0]\n",
    "                \n",
    "        return df\n",
    "\n",
    "\n",
    "    def data_splitter(self, df):\n",
    "        X = df['story']\n",
    "        y = df['highlights']\n",
    "\n",
    "        # Splitting data into train, test and validation subsets\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.20, random_state=42, shuffle=True)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "        return X_train, y_train, X_val, X_test, y_val, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e28cb-fcf8-44b6-987b-05201cb82c57",
   "metadata": {},
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bc7f15f-13a7-4cc6-b338-0c4db1010ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Dataframe: 100%|██████████████████████████████████████████████████████| 21288/21288 [00:06<00:00, 3539.22it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessor = DataPreprocessor()\n",
    "cleaned_data = preprocessor.clean_df(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade50462-a5df-40ac-9729-2e4a54fa3d2d",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c047133-20b2-48e1-8565-9e96422ce7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing Dataframe: 100%|████████████████████████████████████████████████████| 21288/21288 [00:06<00:00, 3389.54it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessor.tokenize(cleaned_data)\n",
    "tokenized_df = preprocessor.df_to_seq(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5a50bc-7ee5-4246-9952-4a141eeda5ce",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0023a72e-5a2b-40ae-a130-8fd2c62934eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding sequences: 100%|███████████████████████████████████████████████████████| 21288/21288 [00:03<00:00, 6957.39it/s]\n"
     ]
    }
   ],
   "source": [
    "padded_df = preprocessor.data_padding(tokenized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee6909-4cb6-46d9-b220-24426b5ce5c6",
   "metadata": {},
   "source": [
    "### Splitting data into subsets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c444a9f-489b-46b9-b971-25c456e6fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, X_test, y_val, y_test = preprocessor.data_splitter(padded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5b569fb-bc0c-40d1-ac7f-16b6394e5298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e824ad1-7ec4-4d55-a7b4-0d6681ebf924",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f1813c-9685-429f-ae7a-3c4c35a6d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model imports\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ea8772e-6548-4272-ab04-146f90694fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 15500 + 1  # 15,500 words + 1 for OOV token\n",
    "EMBEDDING_DIM = 256\n",
    "LSTM_UNITS = 512\n",
    "\n",
    "# Encoder\n",
    "\n",
    "# 1. Input Layer\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 2. Embedding Layer\n",
    "encoder_embedding = Embedding(VOCAB_SIZE, EMBEDDING_DIM)(encoder_inputs)\n",
    "\n",
    "# 3. LSTM Layer\n",
    "encoder_lstm, state_h, state_c = LSTM(LSTM_UNITS, return_state=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "\n",
    "# 1. Input Layer\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 2. Embedding Layer\n",
    "decoder_embedding = Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "dec_emb = decoder_embedding(decoder_inputs)\n",
    "\n",
    "# 3. LSTM Layer\n",
    "decoder_lstm = LSTM(LSTM_UNITS, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\n",
    "# 4. Dense Layer\n",
    "decoder_dense = Dense(VOCAB_SIZE, activation='softmax')\n",
    "output = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Model\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=output)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65c0debe-1a2f-45aa-a951-1843c1fdd40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(y_train[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f02f08a-4e25-4e9e-a39a-174d55be9688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(encoder_input_data.shape)\n",
    "# print(np.array(decoder_input_data).shape)\n",
    "# print(np.array(decoder_target_data).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6477fdd-7952-4636-8199-6c4b1524bc1a",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4cc36dc-39b6-4263-b786-802b7a612104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "178/267 [===================>..........] - ETA: 12:23 - loss: 4.7728 - accuracy: 0.3876"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m decoder_target_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(np\u001b[38;5;241m.\u001b[39marray([seq[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m y_val\u001b[38;5;241m.\u001b[39mtolist()]), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mencoder_input_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Model inputs\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_target_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                       \u001b[49m\u001b[38;5;66;43;03m# Model targets\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mencoder_input_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_target_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Neuraltools\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Neuraltools\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Neuraltools\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Neuraltools\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Neuraltools\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Neuraltools\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Neuraltools\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Neuraltools\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Neuraltools\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prepare the data for training\n",
    "encoder_input_data = np.array(X_train.tolist())  # Encoder input\n",
    "\n",
    "# Decoder input: Exclude the last token\n",
    "decoder_input_data = np.array([seq[:-1] for seq in y_train.tolist()])  \n",
    "\n",
    "# Decoder target: Exclude the first token and add an extra dimension\n",
    "decoder_target_data = np.expand_dims(np.array([seq[1:] for seq in y_train.tolist()]), -1)\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# For validation data, follow a similar approach to get encoder and decoder inputs\n",
    "encoder_input_val = np.array(X_val.tolist())\n",
    "decoder_input_val = np.array([seq[:-1] for seq in y_val.tolist()])\n",
    "# Decoder target for validation: Exclude the first token and add an extra dimension\n",
    "decoder_target_val = np.expand_dims(np.array([seq[1:] for seq in y_val.tolist()]), -1)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [encoder_input_data, decoder_input_data],  # Model inputs\n",
    "    decoder_target_data,                       # Model targets\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=([encoder_input_val, decoder_input_val], decoder_target_val),\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42cb460-98f8-4048-b284-7904cbd1777f",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4c6bb-fe8b-4a25-ae4d-abf717a4d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b83c6a6-6684-4f1b-b093-db5186510241",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665db3d6-b76c-4997-845d-461f48327214",
   "metadata": {},
   "source": [
    "### Word limit exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6774cc-4b98-45e1-bd22-585f21ea7351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Get Word Frequencies\n",
    "# word_freq = preprocessor.tokenizer.word_counts\n",
    "\n",
    "# # Step 2: Sort Word Frequencies in descending order\n",
    "# sorted_word_freq = dict(sorted(word_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# # Step 3: Calculate Cumulative Frequency\n",
    "# word_counts = list(sorted_word_freq.values())\n",
    "# cumulative_word_counts = np.cumsum(word_counts)\n",
    "# total_word_count = cumulative_word_counts[-1]\n",
    "\n",
    "# # Normalize the cumulative sum to get a distribution\n",
    "# cumulative_distribution = cumulative_word_counts / total_word_count\n",
    "\n",
    "# # Step 4: Visualize Cumulative Frequency Distribution\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(cumulative_distribution)\n",
    "# plt.xlabel(\"Number of Unique Words\")\n",
    "# plt.ylabel(\"Cumulative Frequency\")\n",
    "# plt.title(\"Cumulative Distribution of Word Frequencies\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# index_95_percent = np.argmax(cumulative_distribution > 0.95)\n",
    "\n",
    "# # Odpowiadająca wartość słowa\n",
    "# word_at_95_percent = list(sorted_word_freq.keys())[index_95_percent]\n",
    "\n",
    "# print(f\"95% of the cumulative word count is covered by the first {index_95_percent} words.\")\n",
    "# print(f\"The word at the 95% threshold is: {word_at_95_percent}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fceec3-52ce-48ca-b2af-e913457e090e",
   "metadata": {},
   "source": [
    "### Max length exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d2932-58eb-4871-b06e-3c4703d6648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sequence lengths for stories\n",
    "# story_lengths = tokenized_df['story'].apply(len)\n",
    "\n",
    "# # Sequence lengths for highlights\n",
    "# highlight_lengths = tokenized_df['highlights'].apply(len)\n",
    "\n",
    "# # Plotting the length distributions\n",
    "# plt.figure(figsize=(15, 6))\n",
    "\n",
    "# # For highlights\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.hist(highlight_lengths, bins=100, alpha=0.6, color='r', label='Highlights')\n",
    "# plt.xlabel('Length')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Highlights Length Distribution')\n",
    "# plt.legend()\n",
    "\n",
    "# # For stories\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.hist(story_lengths, bins=100, alpha=0.6, color='b', label='Stories')\n",
    "# plt.xlabel('Length')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Stories Length Distribution')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout(pad=4.0)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9471416a-7c4b-46a3-91f0-9508b3d57044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sequence lengths for stories\n",
    "# story_lengths = tokenized_df['story'].apply(len)\n",
    "\n",
    "# # Sequence lengths for highlights\n",
    "# highlight_lengths = tokenized_df['highlights'].apply(lambda x: [len(h) for h in x]).explode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7e8b9e-344c-449e-b037-0dfb9364e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12, 5))\n",
    "\n",
    "# # Histogram for story lengths\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.hist(story_lengths, bins=50, color='blue', alpha=0.7)\n",
    "# plt.title('Distribution of Story Lengths')\n",
    "# plt.xlabel('Story Length')\n",
    "# plt.ylabel('Number of Instances')\n",
    "\n",
    "# # Histogram for highlight lengths\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.hist(highlight_lengths, bins=50, color='green', alpha=0.7)\n",
    "# plt.title('Distribution of Highlight Lengths')\n",
    "# plt.xlabel('Highlight Length')\n",
    "# plt.ylabel('Number of Instances')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c328f-39c2-480d-92d0-22fee934ea5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
